# VLA Research Ideas

ì„ì‚¬ ë…¼ë¬¸ì„ ìœ„í•œ VLA(Vision-Language-Action) ì—°êµ¬ ì•„ì´ë””ì–´ ì •ë¦¬.

## Candidate 1: CC-VLA (Confidence-Conditioned VLA) â­

### Why Novel?
- **Confidence Calibration in VLA (2025.07, arxiv 2507.17383)**: confidenceë¥¼ ë¶„ì„/ë³´ì •ë§Œ í•¨
- ë…¼ë¬¸ ì›ë¬¸: "ìš°ë¦¬ëŠ” ëª¨ë¸ì˜ ë™ì‘ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ë³´ê³ ëœ ì‹ ë¢°ë„ë§Œ ìˆ˜ì •í•©ë‹ˆë‹¤"
- **Gap**: Confidenceë¥¼ action generationì˜ **ì¡°ê±´(condition)**ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì—°êµ¬ ì—†ìŒ!

### Core Formulation

```python
# ê¸°ì¡´ VLA
a = Ï€(o, l)                      # observation, language â†’ action
L = ||a - a*||Â²                  # MSE loss

# CC-VLA (ì œì•ˆ)
c = Ïƒ(MLP_conf(Enc(o, l)))       # Confidence ì˜ˆì¸¡ (0~1)
a = Dec(z, embed(c))             # Confidence-conditioned action generation

# Heteroscedastic Loss
L = ||a - a*||Â² / cÂ² + Î±*log(c) + Î»*BCE(c, success)
#    â†‘ ë¶ˆí™•ì‹¤í•˜ë©´      â†‘ ë„ˆë¬´ ë‚®ì€   â†‘ ì‹¤ì œ ì„±ê³µê³¼
#    loss ê°€ì¤‘ì¹˜ ë‚®ì¶¤   confidence    confidence ì •ë ¬
#                      ë°©ì§€
```

### Key Contributions
1. **Confidence as Explicit Condition**: action generationì— confidenceë¥¼ ëª…ì‹œì  ì…ë ¥ìœ¼ë¡œ
2. **Heteroscedastic Loss**: ë¶ˆí™•ì‹¤í•  ë•Œ ìë™ìœ¼ë¡œ ë³´ìˆ˜ì  í–‰ë™ ìœ ë„
3. **Human Handoff Decision**: confidence thresholdë¡œ ì‚¬ëŒ ê°œì… ì‹œì  ê²°ì •
4. **Calibrated Uncertainty**: post-hocì´ ì•„ë‹Œ í•™ìŠµ ì¤‘ ì§ì ‘ calibration

### Related Work
| Paper | What they do | Gap |
|-------|-------------|-----|
| Confidence Calibration in VLA | Post-hoc confidence ë¶„ì„ | í–‰ë™ ë³€ê²½ ì•ˆí•¨ |
| Heteroscedastic Regression | ì¼ë°˜ íšŒê·€ì—ì„œ uncertainty | VLA ë¯¸ì ìš© |
| Ensemble Methods | ë‹¤ì¤‘ ëª¨ë¸ variance | ê³„ì‚° ë¹„ìš© ë†’ìŒ |

---

## Candidate 2: R-VLA (Reversibility-aware VLA) â­â­

### Why Novel?
- **Safe RLì—ì„œ reversibility ì—°êµ¬**: Googleì˜ RAE/RAC ([Self-Supervised Reversibility-Aware RL](https://research.google/blog/self-supervised-reversibility-aware-reinforcement-learning/))
- **ReVLAëŠ” ë‹¤ë¥¸ ê°œë…**: "reversible training" (ë©”ëª¨ë¦¬ íš¨ìœ¨), action reversibility ì•„ë‹˜
- **Gap**: Actionì˜ reversibilityë¥¼ VLAì˜ **ì¡°ê±´/í•„í„°**ë¡œ ì‚¬ìš©í•˜ëŠ” ì—°êµ¬ ì—†ìŒ!

### Core Formulation

```python
# Reversibility Predictor (Google RAE/RACì—ì„œ ì˜ê°)
# Event Aê°€ B ì „ì— ì¼ì–´ë‚˜ëŠ”ì§€ ì˜ˆì¸¡ â†’ reversibility proxy
r = R_Î¸(o, a)  # reversibility score (0=irreversible, 1=fully reversible)

# Option 1: Action Filtering (RAC ìŠ¤íƒ€ì¼)
while R_Î¸(o, a_sampled) < threshold:
    a_sampled = Ï€(o, l)  # resample until reversible
a = a_sampled

# Option 2: Loss Weighting
L = (1 + Î²*(1-r)) * ||a - a*||Â²
#   â†‘ ë¶ˆê°€ì—­ í–‰ë™ì— ë” ë†’ì€ loss â†’ ë³´ìˆ˜ì  í•™ìŠµ

# Option 3: Conditionìœ¼ë¡œ ì‚¬ìš©
a = Ï€(o, l, embed(r))  # reversibility-aware action
```

### Key Contributions
1. **Safe RL â†’ VLA ì ìš©**: Googleì˜ reversibility ê°œë…ì„ VLAì— ìµœì´ˆ ì ìš©
2. **Action Filtering**: ë¶ˆê°€ì—­ í–‰ë™ ì‚¬ì „ ì°¨ë‹¨
3. **Risk-aware Manipulation**: ë¬¼ì²´ íŒŒì†, ì¶©ëŒ ë“± irreversible ìƒí™© íšŒí”¼
4. **CC-VLAì™€ ìƒí˜¸ë³´ì™„**: CCëŠ” ëª¨ë¸ ë¶ˆí™•ì‹¤ì„±, Rì€ í™˜ê²½ ë¶ˆê°€ì—­ì„±

### vs CC-VLA
| ì¸¡ë©´ | CC-VLA | R-VLA |
|------|--------|-------|
| ì¸¡ì • ëŒ€ìƒ | ëª¨ë¸ì˜ ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± | í™˜ê²½/í–‰ë™ì˜ ë¶ˆê°€ì—­ì„± |
| ê´€ì  | Epistemic (ë‚´ê°€ ëª¨ë¥´ëŠ” ê²ƒ) | Aleatoric (ì„¸ìƒì˜ íŠ¹ì„±) |
| ì‘ìš© | Human handoff | Catastrophe prevention |

---

## Candidate 3: CR-VLA (Confidence + Reversibility VLA) â­â­â­

### Why Novel?
- CC-VLA + R-VLA í†µí•©
- ë‘ ê°€ì§€ orthogonalí•œ safety signal
- **Gap**: ëª¨ë¸ ë¶ˆí™•ì‹¤ì„± + í™˜ê²½ ë¶ˆê°€ì—­ì„±ì„ ë™ì‹œì— ê³ ë ¤í•˜ëŠ” VLA ì—†ìŒ

### Core Formulation

```python
# ë‘ ê°€ì§€ safety score
c = Ïƒ(MLP_conf(Enc(o, l)))    # Confidence (model uncertainty)
r = R_Î¸(o, a)                  # Reversibility (environment property)

# Safety score í†µí•©
safety = c * r  # ë‘˜ ë‹¤ ë†’ì•„ì•¼ ì•ˆì „

# Safety-conditioned action
a = Ï€(o, l, embed(safety))

# Unified Loss
L = ||a - a*||Â² / (c * r)Â² + Î±*log(c) + Î²*log(r) + Î»*BCE(c, success)
```

### Safety Matrix
|              | r ë†’ìŒ (ê°€ì—­) | r ë‚®ìŒ (ë¶ˆê°€ì—­) |
|--------------|-------------|---------------|
| **c ë†’ìŒ (í™•ì‹ )** | âœ… ì‹¤í–‰ | âš ï¸ ì£¼ì˜ ì‹¤í–‰ |
| **c ë‚®ìŒ (ë¶ˆí™•ì‹¤)** | ğŸ”„ íƒìƒ‰ ê°€ëŠ¥ | âŒ ê±°ë¶€/handoff |

### Key Contributions
1. **Dual Safety Signal**: epistemic + aleatoric uncertainty í†µí•©
2. **Principled Decision**: ì–¸ì œ ì‹¤í–‰/ê±°ë¶€/handoff í• ì§€ ëª…í™•í•œ ê¸°ì¤€
3. **Novel Loss Function**: ë‘ safety ìš”ì†Œë¥¼ í†µí•©í•œ heteroscedastic loss

---

## Candidate 4: Social-Intent VLA (SI-VLA)

### Idea
- ì£¼ë³€ ì‚¬ëŒì˜ ì˜ë„(intent) ì˜ˆì¸¡ â†’ í˜‘ë ¥ì  í–‰ë™
- "ì € ì‚¬ëŒì´ ë­˜ í•˜ë ¤ëŠ”ì§€" ì´í•´í•˜ê³  ë•ê¸°

### Status: âš ï¸ ë¶€ë¶„ í¬í™”
- Human intent prediction for HRI: ì´ë¯¸ ì—°êµ¬ í™œë°œ ([Frontiers article](https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1708987/full))
- Figure Helix: ë‘ ë¡œë´‡ì´ í˜‘ë ¥í•˜ëŠ” ë°ëª¨ ì¡´ì¬
- VLAì— ì§ì ‘ í†µí•©ì€ gap ìˆì„ ìˆ˜ ìˆìœ¼ë‚˜, ì—°êµ¬ ë°©í–¥ ê²¹ì¹¨

---

## Candidate 5: Modular Skill VLA (MS-VLA)

### Idea
- Taskë¥¼ skillë¡œ ë¶„í•´ â†’ skill composition
- "Pick and place" = Pick + Place

### Status: âŒ í¬í™”
- NVIDIA GR00T N1: System 1 + System 2 dual architecture
- Agentic VLA: LLM planner + VLA skills
- Hierarchical decomposition ì´ë¯¸ í™œë°œ

---

## Exploration Log

### 2026-02-06: ì´ˆê¸° íƒìƒ‰
- Anticipatory VLA â†’ UP-VLA, HiF-VLA ì¡´ì¬ (Red Ocean)
- Memory-Augmented â†’ MemoryVLA, MAP-VLA, EchoVLA (Saturated)
- Cross-Embodiment â†’ ET-VLA, X-VLA (Saturated)

### ë°œê²¬ëœ Gap (2ì°¨ ê²€ì¦ ì™„ë£Œ)
1. **Confidence Conditioning** â† CC-VLA â­ (ê°€ì¥ ì„ ëª…í•œ gap)
2. **Reversibility-aware** â† R-VLA (VLA ë¯¸ì ìš© í™•ì¸)
3. **Confidence + Reversibility í†µí•©** â† CR-VLA (Safety VLA ê²½ìŸ ì¹˜ì—´)

### í¬í™”ëœ ë°©í–¥
- Style/Personalization â†’ GRAPE (2024.11)ê°€ ì´ë¯¸ preference alignment
- Skill Composition â†’ GR00T N1, Agentic VLA
- Human Intent â†’ HRI ë¶„ì•¼ì—ì„œ í™œë°œ
- CoT Reasoning â†’ CoT-VLA, FlowVLA, CoA-VLA ë‹¤ìˆ˜
- **ì¶”ê°€ Modality â†’ ì™„ì „ í¬í™”!** (ì•„ë˜ ì°¸ì¡°)

### ì¶”ê°€ Modality í˜„í™© (2025-2026, ì „ë¶€ ìˆìŒ!)
| Modality | ë…¼ë¬¸ | ë¹„ê³  |
|----------|------|------|
| Tactile | Tactile-VLA | ì´‰ê° ì„¼ì„œ |
| Audio | Audio-VLA | ì ‘ì´‰ ì†Œë¦¬ |
| Force/Torque | ForceVLA, TA-VLA | 6ì¶• í˜ì„¼ì„œ, í† í¬ |
| Speech | SVA (Speech-VLA) | ìŒì„± ëª…ë ¹ |
| Depth | ë‹¤ìˆ˜ | Point cloud í†µí•© |
| Thermal | OmniSegmentor | ì—´í™”ìƒ |

### Safety VLA ê²½ìŸ í˜„í™©
| ë…¼ë¬¸ | ì ‘ê·¼ ë°©ì‹ | CC-VLAì™€ì˜ ì°¨ì´ |
|------|----------|----------------|
| SafeVLA (NeurIPS 2025 Spotlight) | CMDP, unsafe ë¶„ë¥˜ | Binary constraint |
| VLSA/AEGIS | Control Barrier Function | Physics constraint |
| CompliantVLA | Variable Impedance | ì ‘ì´‰ ìˆœì‘ |
| Confidence Cal (2507.17383) | Post-hoc ë³´ì •ë§Œ | í–‰ë™ ë³€ê²½ ì•ˆí•¨ |
| **CC-VLA (ì œì•ˆ)** | **Heteroscedastic conditioning** | **Continuous self-awareness** |

---

## ìµœì¢… ì¶”ì²œ ìˆœìœ„ (4ì°¨ ê²€ì¦ - Manipulation íŠ¹í™” ì¬íƒìƒ‰)

### 4ì°¨ ê²€ì¦ ê²°ë¡ : VLA ë¶„ì•¼ëŠ” ê±°ì˜ ëª¨ë“  ë°©í–¥ì´ íƒìƒ‰ë¨
2025-2026ì— í­ë°œì ìœ¼ë¡œ ì„±ì¥í•˜ì—¬ "___-VLA" í˜•íƒœì˜ ëŒ€ë¶€ë¶„ì˜ ì•„ì´ë””ì–´ê°€ ì´ë¯¸ ì¡´ì¬.
ì™„ì „íˆ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ë³´ë‹¤ëŠ” **ê¸°ì¡´ ê¸°ë²•ì˜ ê¹Šì´ ìˆëŠ” ë¶„ì„ + ìƒˆë¡œìš´ ì¡°í•© + ì‹¤ì œ ë¡œë´‡ ì‹¤í—˜**ì´ í˜„ì‹¤ì .

### 4ì°¨ íƒìƒ‰ì—ì„œ í™•ì¸í•œ ì¶”ê°€ í¬í™” ë°©í–¥
| ë°©í–¥ | ê¸°ì¡´ ì—°êµ¬ | ìƒíƒœ |
|------|----------|------|
| Object-Centric | Oat-VLA, OmniManip (CVPR 2025), ObjectVLA | âŒ í¬í™” |
| 3D Spatial | SpatialVLA, GraphCoT-VLA, Spatial Forcing | âŒ í¬í™” |
| World Model | WorldVLA, Cosmos Policy, WALL-A | âŒ í¬í™” |
| Failure Recovery | FailSafe-VLM | âŒ ì¡´ì¬ |
| Dynamic Object | DynamicVLA (2601.22153) | âŒ ì¡´ì¬ |
| Grasping íŠ¹í™” | GraspVLA, VLA-Grasp, DexGraspVLA | âŒ í¬í™” |
| Tactile+VLA | VTLA (peg insertion), Tactile-VLA | âŒ í¬í™” |
| Ensemble/Voting | VOTE (2507.05116), PI-VLA AURD | âŒ ì¡´ì¬ |
| Tree of Thoughts | Embodied ToT (2512.08188), MCTS | âŒ ì¡´ì¬ |
| LoRA Fine-tuning | LoRA-VLA (2512.11921), OpenVLA-OFT | âŒ í¬í™” |
| RL Fine-tuning | VLA-RL, VLA-RFT | âŒ í¬í™” |
| Action Chunking | PD-VLA, FAST, RTC, VLA-Cache | âŒ í¬í™” |
| Counterfactual | RoCoDA (data augmentationë§Œ) | âš ï¸ VLA ì§ì ‘ ì ìš©ì€ ì—†ìŒ |

### CC-VLA ì•½ì  ì¸ì • (3ì°¨ì—ì„œ ìœ ì§€)
- Heteroscedastic regressionì€ ì´ë¯¸ ì˜ ì•Œë ¤ì§„ ê¸°ë²• (Kendall & Gal, 2017)
- "ê¸°ë²• ì ìš©" ìˆ˜ì¤€ì´ì§€ "ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„"ì´ë¼ í•˜ê¸°ì—” ì•½í•¨
- Reviewer: "SafeVLAë‘ ë­ê°€ ë‹¤ë¦„?" ì— ëŒ€í•œ ëª…ì¾Œí•œ ë‹µ í•„ìš”
- PI-VLAì˜ AURDë„ action disagreementë¡œ uncertainty ê°ì§€ (ìœ ì‚¬)
- **ë‹¨ë…ìœ¼ë¡œëŠ” top venue ì–´ë ¤ì›€, ë³´ê°• í•„ìš”**

### 3ì°¨ íƒìƒ‰ì—ì„œ ì¶”ê°€ë¡œ í¬í™” í™•ì¸ëœ ë°©í–¥
| ë°©í–¥ | ê¸°ì¡´ ì—°êµ¬ | ìƒíƒœ |
|------|----------|------|
| Test-Time Adaptation | TT-VLA, HyperVLA, PLD | âŒ í¬í™” |
| Test-Time Compute Scaling | SITCOM, Dynamic TTC | âŒ í¬í™” |
| Language-to-Impedance | HumanoidVLM, OmniVIC, ImpedanceGPT | âŒ í¬í™” |
| In-Context Learning | MoS-VLA, RoboPrompt, DEMONSTRATE | âš ï¸ í™œë°œ |
| Data Quality | Consistency Matters, PLD, EMMA | âš ï¸ ì‹œì‘ë¨ but gap ìˆìŒ |

### ICLR 2026 ì „ë¬¸ê°€ê°€ ì§€ì í•œ Gap (Moritz Reuss ë¸”ë¡œê·¸)
1. **Data Quality**: "surprisingly few submissions focused on data curation"
2. **In-Context Learning**: "expected more work here but found almost none"
â†’ í•˜ì§€ë§Œ 2ì°¨ ê²€ìƒ‰ì—ì„œ ICL ë…¼ë¬¸ ë‹¤ìˆ˜ ë°œê²¬ (MoS-VLA ë“±)

---

## í˜„ì‹¤ì  ì¶”ì²œ ìˆœìœ„ (4ì°¨ ìµœì¢…)

| ìˆœìœ„ | í›„ë³´ | ê°•ì  | ì•½ì  | ëª©í‘œ venue |
|------|------|------|------|-----------|
| **1** | **CC-VLA â†’ SA-VLA (Self-Aware VLA)** | ëª…í™•í•œ gap, êµ¬í˜„ ê°€ëŠ¥, í”„ë ˆì„ì›Œí¬ë¡œ í™•ì¥ | SafeVLA/PI-VLAì™€ ì°¨ë³„í™” í•„ìˆ˜ | Journal / Workshop |
| **2** | **DQ-VLA (Data Quality)** | ì „ë¬¸ê°€ ì¸ì • gap, ì‹¤í—˜ ë°”ë¡œ ê°€ëŠ¥ | "Consistency Matters"ì™€ ì°¨ë³„í™” í•„ìš” | Conference / Journal |
| 3 | **CA-VLA (Counterfactual-Augmented)** | VLA ì§ì ‘ ì ìš© ì—†ìŒ (gap!) | ì‹œë®¬ë ˆì´í„° í•„ìš”, êµ¬í˜„ ë³µì¡ | Conference |
| 4 | R-VLA | VLA ë¯¸ì ìš© í™•ì¸ | ì‹¤í—˜ ë°ì´í„° êµ¬ì¶• ì–´ë ¤ì›€ | Journal |

### 1ìœ„: SA-VLA (Self-Aware VLA) - CC-VLAì˜ í™•ì¥

CC-VLAë¥¼ ë‹¨ìˆœ confidence ë…¼ë¬¸ì´ ì•„ë‹Œ **ì¢…í•© í”„ë ˆì„ì›Œí¬**ë¡œ í™•ì¥:

```python
# SA-VLA: Self-Aware VLA Framework

# Module 1: Confidence Estimation (CC-VLA í•µì‹¬)
c = Ïƒ(MLP_conf(Enc(o, l)))       # Confidence (0~1)

# Module 2: Phase Detection
phase = PhaseClassifier(o, l)     # approach/align/grasp/transport/place

# Module 3: OOD Detection
ood_score = MahalanobisOOD(z)     # í•™ìŠµ ë¶„í¬ì—ì„œ ë²—ì–´ë‚œ ì •ë„

# Module 4: Unified Safety Score
safety = f(c, phase, ood_score)   # í†µí•© safety score

# Module 5: Adaptive Action Generation
if safety > Ï„_execute:
    a = Ï€(o, l, embed(safety))    # ì •ìƒ ì‹¤í–‰
elif safety > Ï„_cautious:
    a = Ï€_slow(o, l, embed(safety))  # ë³´ìˆ˜ì  ì‹¤í–‰ (ëŠë¦° ì†ë„)
else:
    a = STOP + request_human_help()   # ê±°ë¶€ + ì‚¬ëŒ ìš”ì²­

# Heteroscedastic Training Loss
L = ||a - a*||Â² / cÂ² + Î±*log(c) + Î»*BCE(c, success) + Î³*CE(phase, phase*)
```

**SafeVLA/PI-VLAì™€ì˜ ì°¨ë³„í™”:**
| ì¸¡ë©´ | SafeVLA | PI-VLA | SA-VLA (ì œì•ˆ) |
|------|---------|--------|--------------|
| ì•ˆì „ ì‹ í˜¸ | Binary (safe/unsafe) ë¶„ë¥˜ | Action disagreement | Continuous confidence |
| ë°©ë²• | CMDP constraint | Multiple sampling + voting | Single-pass estimation |
| ë¹„ìš© | ë³„ë„ safety classifier í•„ìš” | Multiple forward passes | ë‹¨ì¼ forward pass |
| Phase ì¸ì‹ | ì—†ìŒ | Symmetry-aware | Phase-explicit |
| OOD ê°ì§€ | ì—†ìŒ | ì—†ìŒ | Mahalanobis distance |
| í•µì‹¬ ì°¨ì´ | ì™¸ë¶€ ì œì•½ (external) | ë‹¤ì¤‘ ìƒ˜í”Œ í•©ì˜ | ë‚´ë¶€ ìê° (internal) |

### 2ìœ„: DQ-VLA (Data Quality-aware VLA)

```python
# ê¸°ì¡´: ëª¨ë“  ë°ëª¨ë¥¼ ë™ë“±í•˜ê²Œ í•™ìŠµ
L = Î£ ||a_i - a*_i||Â²

# DQ-VLA: ë°ëª¨ í’ˆì§ˆì— ë”°ë¼ ê°€ì¤‘ í•™ìŠµ
q_i = QualityScorer(demo_i)      # ë°ëª¨ í’ˆì§ˆ ìë™ í‰ê°€
L = Î£ q_i * ||a_i - a*_i||Â²     # ê³ í’ˆì§ˆ ë°ëª¨ì—ì„œ ë” ë§ì´ í•™ìŠµ

# + Curriculum: ì‰¬ìš´(ê³ í’ˆì§ˆ) â†’ ì–´ë ¤ìš´(ì €í’ˆì§ˆ) ìˆœì„œë¡œ
```

ì°¨ë³„í™” vs "Consistency Matters":
- ê·¸ë“¤: metric ì •ì˜ì— ì´ˆì 
- DQ-VLA: metric â†’ ì‹¤ì œ í•™ìŠµì— ë°˜ì˜ (quality-weighted training)

ì°¨ë³„í™” vs PLD:
- PLD: RLë¡œ ìƒˆ ë°ì´í„° ìë™ ìƒì„±
- DQ-VLA: ê¸°ì¡´ ë°ì´í„°ì˜ í’ˆì§ˆ ê¸°ë°˜ ì»¤ë¦¬í˜ëŸ¼

ì°¨ë³„í™” vs EMMA:
- EMMA: challenging sample reweighting (ì–´ë ¤ìš´ ìƒ˜í”Œ ê°€ì¤‘)
- DQ-VLA: quality scoring + curriculum (í’ˆì§ˆ ì¸¡ì • + ìˆœì„œ í•™ìŠµ)

### 3ìœ„: CA-VLA (Counterfactual-Augmented VLA)

```python
# ê¸°ì¡´: demonstration ê·¸ëŒ€ë¡œ í•™ìŠµ
(o_t, a_t, o_{t+1}) â†’ L = ||Ï€(o_t) - a_t||Â²

# CA-VLA: counterfactual ê²½í—˜ë„ í•™ìŠµ
# Step 1: ì›ë˜ trajectoryì—ì„œ actionì„ perturbation
a_cf = a_t + Î´                    # counterfactual action

# Step 2: ì‹œë®¬ë ˆì´í„°/world modelë¡œ ê²°ê³¼ ì˜ˆì¸¡
o_cf = WorldModel(o_t, a_cf)      # counterfactual outcome

# Step 3: ì›ë˜ vs counterfactual ë¹„êµ í•™ìŠµ
L = ||Ï€(o_t) - a_t||Â² + Î² * max(0, margin - d(o_{t+1}, o_cf))
#   ì›ë˜ í–‰ë™ ëª¨ë°©       + counterfactualì´ ë‚˜ì˜ë‹¤ëŠ” ê²ƒë„ í•™ìŠµ
```

ì°¨ë³„í™” vs RoCoDA:
- RoCoDA: scene-level augmentation (ë°°ê²½, ë¬¼ì²´ ìœ„ì¹˜ ë³€ê²½)
- CA-VLA: action-level counterfactual (í–‰ë™ ë³€ê²½ì˜ ê²°ê³¼ í•™ìŠµ)

---

## Exploration Log

### 2026-02-06: 4ì°¨ íƒìƒ‰ (Manipulation íŠ¹í™”)
ê²€ìƒ‰ ë²”ìœ„ë¥¼ "ë¡œë´‡ íŒ” manipulation + VLA"ë¡œ ì¢í˜€ì„œ ì¬íƒìƒ‰.
- Object-centric, Spatial, World Model, Grasping, Dynamic â†’ ì „ë¶€ ì¡´ì¬
- Action chunking, LoRA fine-tuning, RL fine-tuning â†’ ì „ë¶€ í¬í™”
- Counterfactual reasoning: VLA ì§ì ‘ ì ìš©ì€ ì—†ìŒ â†’ CA-VLA gap í™•ì¸
- PI-VLA (AURD): action disagreement ê¸°ë°˜ uncertainty ê°ì§€ â†’ CC-VLAì™€ ìœ ì‚¬ì  ë°œê²¬
- VOTE: ensemble voting â†’ Self-Consistency ì•„ì´ë””ì–´ ì´ë¯¸ ì¡´ì¬
- Embodied ToT: Tree of Thoughts ë¡œë´‡ ì ìš© ì´ë¯¸ ì¡´ì¬
- ìµœì¢… ê²°ë¡ : ì™„ì „íˆ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ ì–´ë ¤ì›€, **ê¹Šì´ ìˆëŠ” í”„ë ˆì„ì›Œí¬ + ì‹¤í—˜**ì´ í˜„ì‹¤ì 

---

## 5ì°¨ íƒìƒ‰: 3DGS + CG/XR + Robot êµì°¨ ë¶„ì•¼ (2026-02-06)

### ë°°ê²½
êµìˆ˜ë‹˜ ë¶„ì•¼(Gaussian Splatting, Computer Graphics, XR)ì™€ ë¡œë´‡ manipulationì„ ê²°í•©í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì „í™˜.
VLA ìì²´ì˜ noveltyëŠ” í¬í™” ìƒíƒœì´ë¯€ë¡œ, **3DGS/CG/XR ê¸°ìˆ ì„ ë¡œë´‡ì— ì ìš©**í•˜ëŠ” êµì°¨ ì—°êµ¬ë¥¼ íƒìƒ‰.

### 3DGS + Robot ê¸°ì¡´ ì—°êµ¬ ì§€ë„

| ë°©í–¥ | ê¸°ì¡´ ì—°êµ¬ | í¬í™”ë„ |
|------|----------|--------|
| 3DGS â†’ Grasping/Affordance | GaussianGrasper, GraspSplats (CoRL'24), Splat-MOVER | âš ï¸ í™œë°œ |
| 3DGS â†’ Data Augmentation | RoboSplat (RSS'25), R2R2R (CoRL'25) | âš ï¸ í™œë°œ |
| 3DGS â†’ World Model | GWM (ICCV'25), ManiGaussian (ECCV'24) | âš ï¸ ì¡´ì¬ |
| 3DGS â†’ Sim2Real | SplatSim (ICRA'25), RoboGSim | âš ï¸ ì¡´ì¬ |
| 3DGS â†’ VR Teleoperation | Human-in-the-Loop GS (RAL'25) | âš ï¸ ì¡´ì¬ |
| 3DGS â†’ Policy Evaluation | Real-to-Sim Policy Eval with GS (2511.04665) | âš ï¸ ì¡´ì¬ |
| 3DGS â†’ RL Representation | GSRL | âš ï¸ ì‹œì‘ë¨ |
| 3DGS â†’ Self-Correction | GS-Splatted Foresight (AAAI'25) | âš ï¸ ì¡´ì¬ |
| 3DGS â†’ Object Tracking | POGS, Object-Aware GS | âš ï¸ ì¡´ì¬ |
| Single-view 3DGS | SVG3D, SPAGS, SIGMA | âš ï¸ í™œë°œ |
| 3DGS â†’ Long-term Service | GS-LTS | ì¡´ì¬ |
| 3DGS â†’ SLAM | SemGauss-SLAM (IROS'25), RGBDS-SLAM (RAL'25) | âš ï¸ í™œë°œ |

### 5ì°¨ ìµœì¢… í›„ë³´

#### 1ìœ„: Depth-GS-Aug (Depth-Guided 3DGS for Low-Cost Robot Data Augmentation)

**í•µì‹¬**: Azure Kinect 1ëŒ€ (RGB-D) â†’ depth-guided 3DGS ì¬êµ¬ì„± â†’ novel view + object/lighting variation â†’ policy training data ì¦ê°•

```
[Azure Kinect RGB-D] â†’ [Depth-Guided 3DGS] â†’ [Scene Editing] â†’ [Novel Views] â†’ [Policy Training]
   single camera         few-shot recon        object/light       augmented       improved
   (ìš°ë¦¬ ì¥ë¹„)            depth prior í™œìš©       variation          demonstrations   performance
```

**ì°¨ë³„í™”:**
| ì¸¡ë©´ | R2R2R (Berkeley) | RoboSplat (RSS'25) | Depth-GS-Aug (ì œì•ˆ) |
|------|-----|---------|---------|
| Input | Multi-view smartphone scan | Multi-view images | **Single RGB-D camera** |
| ë¬¼ë¦¬ ì‹œë®¬ë ˆì´í„° | IsaacLab í•„ìš” | ë¶ˆí•„ìš” | **ë¶ˆí•„ìš”** |
| Object scan | ë³„ë„ í•„ìš” | ë¶ˆí•„ìš” | **ë¶ˆí•„ìš”** |
| íƒ€ê²Ÿ | ëŒ€í˜• ë© | ëŒ€í˜• ë© | **ì†Œê·œëª¨ ì—°êµ¬ì‹¤** |
| Depth í™œìš© | ê°„ì ‘ì  | ì—†ìŒ | **Depth prior for few-shot 3DGS** |

**êµ¬í˜„ íŒŒì´í”„ë¼ì¸:**
1. Azure Kinectë¡œ manipulation scene ì´¬ì˜ (RGB + Depth)
2. Depth-guided few-shot 3DGS reconstruction (SVG3D/FSGS ê¸°ë°˜)
3. Scene editing: object pose randomization, lighting variation, background change
4. Novel view rendering â†’ synthetic demonstration ìƒì„±
5. ì›ë³¸ ë°ì´í„° + augmented ë°ì´í„°ë¡œ policy í•™ìŠµ (Diffusion Policy or VLA)
6. ì‹¤ì œ ë¡œë´‡ì—ì„œ ì„±ëŠ¥ ë¹„êµ í‰ê°€

**êµìˆ˜ë‹˜ ë¶„ì•¼ ì í•©ë„**: â˜…â˜…â˜…â˜…â˜† (3DGS + rendering + depth reconstruction)
**êµ¬í˜„ ë‚œì´ë„**: â˜…â˜…â˜…â˜†â˜† (ê¸°ì¡´ 3DGS ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš© ê°€ëŠ¥)
**ì¥ë¹„ ì í•©**: â˜…â˜…â˜…â˜…â˜… (Azure Kinect, RTX 4070 Ti, RoArm M3 ëª¨ë‘ í™œìš©)

#### 2ìœ„: GS-Progress (3DGS-based Manipulation Task Progress Estimation)

**í•µì‹¬**: manipulation ì‘ì—…ì˜ before/during/afterë¥¼ 3DGSë¡œ ë³µì› â†’ 3D geometry ë³€í™” â†’ task progress metric

```
[Scene t=0] â†’ [3DGS_0] â”€â”
[Scene t=T] â†’ [3DGS_T] â”€â”¤â†’ [Gaussian Distance] â†’ [Progress Score]
                          â””â†’ [3D Change Map]    â†’ [Visualization]
```

**ì°¨ë³„í™”:**
- ê¸°ì¡´: 2D image change detection for task eval (ë§ìŒ)
- ê¸°ì¡´: Real-to-Sim Policy Eval (2511.04665): soft-body íŠ¹í™”
- ì œì•ˆ: **3DGS geometry ë³€í™”ëŸ‰ìœ¼ë¡œ general manipulation progressë¥¼ ì •ëŸ‰í™”**

**êµìˆ˜ë‹˜ ë¶„ì•¼ ì í•©ë„**: â˜…â˜…â˜…â˜…â˜† (3DGS scene understanding)
**êµ¬í˜„ ë‚œì´ë„**: â˜…â˜…â˜†â˜†â˜† (ë¹„êµì  ê°„ë‹¨)
**ë…¼ë¬¸ ì„íŒ©íŠ¸**: â˜…â˜…â˜…â˜†â˜† (metric ë…¼ë¬¸, contribution ì‘ì„ ìˆ˜ ìˆìŒ)

#### 3ìœ„: GS-XR-Demo (3DGS + XR for Interactive Robot Demo Generation)

**í•µì‹¬**: 3DGSë¡œ í™˜ê²½ ë³µì› â†’ XR(AR/VR)ì—ì„œ ì‚¬ìš©ìê°€ scene í¸ì§‘ â†’ í¸ì§‘ëœ 3DGS â†’ ìƒˆ demonstration ìƒì„±

```
[Real Scene] â†’ [3DGS Recon] â†’ [XR Environment] â†’ [Interactive Edit] â†’ [New Demo]
 Azure Kinect    ë³µì›            HMD/AR display    ë¬¼ì²´ ì´ë™/ì¶”ê°€       policy í•™ìŠµìš©
```

**ì°¨ë³„í™”:**
- RoboSplat: ìë™ augmentation
- Human-in-the-Loop GS (RAL'25): real-time teleop (ê°™ì€ ê³µê°„)
- **ì œì•ˆ: interactive XR editingìœ¼ë¡œ diverse demonstration ìƒì„± (ì›ê²© ê°€ëŠ¥)**

**êµìˆ˜ë‹˜ ë¶„ì•¼ ì í•©ë„**: â˜…â˜…â˜…â˜…â˜… (XR + 3DGS ì™„ë²½ ë§¤ì¹­)
**êµ¬í˜„ ë‚œì´ë„**: â˜…â˜…â˜…â˜…â˜† (XR í—¤ë“œì…‹ í•„ìš”, êµ¬í˜„ ë³µì¡)
**ë…¼ë¬¸ ì„íŒ©íŠ¸**: â˜…â˜…â˜…â˜…â˜† (êµì°¨ ë¶„ì•¼ novelty ë†’ìŒ)
**ë¦¬ìŠ¤í¬**: XR ì¥ë¹„ ë³´ìœ  ì—¬ë¶€, êµ¬í˜„ ì‹œê°„

### 5ì°¨ íƒìƒ‰ ê²°ë¡ 

VLA ìì²´ë³´ë‹¤ **3DGS/CG + Robot êµì°¨ ë¶„ì•¼**ê°€ ë” í˜„ì‹¤ì :
1. êµìˆ˜ë‹˜ì´ 3DGS/XR ì „ë¬¸ê°€ì´ë¯€ë¡œ ì§€ë„ ê°€ëŠ¥
2. 3DGS + Robotì€ VLA ëŒ€ë¹„ ì•„ì§ êµì°¨ ì¡°í•©ì˜ ì—¬ì§€ ìˆìŒ
3. ë³´ìœ  ì¥ë¹„(Azure Kinect, RTX 4070 Ti, RoArm M3)ë¥¼ ëª¨ë‘ í™œìš© ê°€ëŠ¥
4. ì¡¸ì—… ë…¼ë¬¸ ìˆ˜ì¤€ìœ¼ë¡œ ì ì ˆí•œ scope

**êµìˆ˜ë‹˜ê³¼ ìƒì˜í•  ë•Œ ì œì•ˆ ìˆœì„œ:**
1. êµìˆ˜ë‹˜ì´ XR ì¥ë¹„ ìˆìœ¼ë©´ â†’ GS-XR-Demo (3ìœ„ì§€ë§Œ êµìˆ˜ë‹˜ ë¶„ì•¼ ìµœì )
2. XR ì¥ë¹„ ì—†ìœ¼ë©´ â†’ Depth-GS-Aug (1ìœ„, ê°€ì¥ ì•ˆì „)
3. ë¹ ë¥¸ ë…¼ë¬¸ í•„ìš” â†’ GS-Progress (2ìœ„, êµ¬í˜„ ê°€ì¥ ë¹ ë¦„)

---

## 5ì°¨ íƒìƒ‰ References (3DGS + Robot)

### Data Augmentation / Sim2Real
- RoboSplat (RSS 2025): [arxiv 2504.13175](https://arxiv.org/abs/2504.13175)
- Real2Render2Real (CoRL 2025): [arxiv 2505.09601](https://arxiv.org/abs/2505.09601)
- SplatSim (ICRA 2025): [arxiv 2409.10161](https://arxiv.org/abs/2409.10161)
- RoboGSim: [arxiv 2411.11839](https://arxiv.org/abs/2411.11839)

### World Model / Scene Understanding
- GWM (ICCV 2025): [ICCV PDF](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_GWM_Towards_Scalable_Gaussian_World_Models_for_Robotic_Manipulation_ICCV_2025_paper.pdf)
- ManiGaussian (ECCV 2024): [GitHub](https://github.com/GuanxingLu/ManiGaussian)
- Self-Correcting via GS Foresight (AAAI): [AAAI](https://ojs.aaai.org/index.php/AAAI/article/view/34866)
- SceneSplat (ICCV 2025): [ICCV PDF](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining_ICCV_2025_paper.pdf)

### Grasping / Affordance
- GaussianGrasper: [arxiv 2403.09637](https://arxiv.org/abs/2403.09637)
- Splat-MOVER: [arxiv 2405.04378](https://arxiv.org/abs/2405.04378)
- POGS: [Berkeley PDF](https://autolab.berkeley.edu/assets/publications/media/2025-ICRA-POGS-CRv5.pdf)

### Teleoperation / XR
- Human-in-the-Loop GS Teleoperation (RAL 2025)
- Communication Efficient Robotic MR with GS: [arxiv 2508.08624](https://arxiv.org/abs/2508.08624)
- OpenVR Teleoperation: [ScienceDirect](https://www.sciencedirect.com/science/article/pii/S2352711025000214)
- ScaleGS (XR rendering): [ACM TACO](https://dl.acm.org/doi/10.1145/3774425)

### Policy Learning / RL
- GSRL: [arxiv 2404.07950](https://arxiv.org/abs/2404.07950)
- 3D Diffusion Policy (DP3, RSS 2024): [arxiv 2403.03954](https://arxiv.org/abs/2403.03954)
- DP4 (ICCV 2025): [ICCV PDF](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Spatial-Temporal_Aware_Visuomotor_Diffusion_Policy_Learning_ICCV_2025_paper.pdf)
- Real-to-Sim Policy Eval with GS: [arxiv 2511.04665](https://arxiv.org/abs/2511.04665)

### Single/Few-shot 3DGS
- SVG3D: [Nature Scientific Reports](https://www.nature.com/articles/s41598-025-03200-7)
- SPAGS: [arxiv 2511.17092](https://arxiv.org/abs/2511.17092)
- FSGS: [Springer](https://link.springer.com/chapter/10.1007/978-3-031-72933-1_9)
- Next Best Sense (Stanford): [Stanford ARM](https://arm.stanford.edu/next-best-sense)

### SLAM
- SemGauss-SLAM (IROS 2025)
- RGBDS-SLAM (RAL 2025)
- Multi-robot 3D recon with GS (RAL 2025)

### Surveys
- 3DGS in Robotics Survey: [arxiv 2410.12262](https://arxiv.org/abs/2410.12262)
- Awesome 3DGS in Robotics: [GitHub](https://github.com/zstsandy/Awesome-3D-Gaussian-Splatting-in-Robotics)
- Radiance Fields in XR Survey: [arxiv 2508.04326](https://arxiv.org/abs/2508.04326)

---

## References
- Confidence Calibration in VLA: arxiv 2507.17383
- Evaluating Uncertainty in VLA: arxiv 2507.17049
- UP-VLA: arxiv 2501.18867
- HiF-VLA: arxiv 2512.09928
- GRAPE (Preference Alignment): arxiv 2411.19309
- SafeVLA (NeurIPS 2025): arxiv 2503.03480
- VLSA/AEGIS (Safety Constraint): arxiv 2512.11891
- CompliantVLA: arxiv 2601.15541
- VLAC (Critic): arxiv 2509.15937
- Tactile-VLA: arxiv 2507.09160
- Audio-VLA: arxiv 2511.09958
- ForceVLA: OpenReview
- TA-VLA (Torque-aware): arxiv 2509.07962
- SVA (Speech-VLA): ScienceDirect
- CRT (Corruption Restoration): arxiv 2602.01158
- Google RAE/RAC: https://research.google/blog/self-supervised-reversibility-aware-reinforcement-learning/
- GR00T N1: NVIDIA dual-system VLA
