# Isaac Lab GPU 병렬 처리 & PPO 알고리즘 상세 설명

> RoArm-M3-Pro 프로젝트의 RL 학습 파이프라인 기술 문서
> 최종 업데이트: 2026-01-12

---

## 목차

1. [GPU vs CPU 기본 개념](#1-gpu-vs-cpu-기본-개념)
2. [PhysX 물리 엔진](#2-physx-물리-엔진)
3. [Warp JIT 컴파일](#3-warp-jit-컴파일)
4. [Isaac Lab 통합 구조](#4-isaac-lab-통합-구조)
5. [PPO 알고리즘](#5-ppo-알고리즘)
6. [우리 환경 설정](#6-우리-환경-설정)
7. [예상 질문 & 답변](#7-예상-질문--답변)

---

## 1. GPU vs CPU 기본 개념

### 전체 그림

```
┌─────────────────────────────────────────────────────────────────────────┐
│  "로봇 16개를 동시에 시뮬레이션하려면 어떻게 해야 할까?"                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  방법 1: CPU로 순차 처리 (느림)                                         │
│  ┌─────┐ → ┌─────┐ → ┌─────┐ → ... (하나씩 계산)                       │
│  │Robot│   │Robot│   │Robot│       총 시간: 16 × 단위시간              │
│  └─────┘   └─────┘   └─────┘                                           │
│                                                                         │
│  방법 2: GPU로 병렬 처리 (빠름)                                         │
│  ┌─────┬─────┬─────┬─────┐                                             │
│  │Robot│Robot│Robot│Robot│  (동시에 계산)                              │
│  │Robot│Robot│Robot│Robot│  총 시간: 1 × 단위시간                      │
│  │Robot│Robot│Robot│Robot│                                             │
│  │Robot│Robot│Robot│Robot│                                             │
│  └─────┴─────┴─────┴─────┘                                             │
│                                                                         │
│  → Isaac Lab은 방법 2를 사용                                            │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### CPU (Central Processing Unit)

```
┌─────────────────────────────────────────────────────────────────────────┐
│  CPU = "천재 1명"                                                       │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  특징:                                                                  │
│  ├── 코어 수: 8~24개 (적음)                                             │
│  ├── 각 코어: 매우 똑똑함 (복잡한 계산 가능)                            │
│  ├── 클럭: 3~5GHz (빠름)                                                │
│  └── 적합한 작업: 복잡한 로직, 분기 많은 코드                           │
│                                                                         │
│  비유: 수학 천재 1명이 문제 100개를 순서대로 푸는 것                    │
│        → 한 문제당 10초 × 100개 = 1000초                               │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### GPU (Graphics Processing Unit)

```
┌─────────────────────────────────────────────────────────────────────────┐
│  GPU = "평범한 일꾼 10,000명"                                           │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  특징:                                                                  │
│  ├── 코어 수: 수천~만 개 (매우 많음)                                    │
│  │   └── RTX 4070 Ti SUPER: 8,448 CUDA 코어                            │
│  ├── 각 코어: 단순한 계산만 가능                                        │
│  ├── 클럭: 2~2.5GHz (CPU보다 느림)                                      │
│  └── 적합한 작업: 같은 계산을 여러 데이터에 동시 적용                   │
│                                                                         │
│  비유: 평범한 학생 100명이 같은 유형의 문제를 동시에 푸는 것            │
│        → 한 문제당 10초 × 1개 (동시) = 10초                            │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 왜 로봇 시뮬레이션에 GPU가 좋은가?

```
┌─────────────────────────────────────────────────────────────────────────┐
│  로봇 시뮬레이션의 특징                                                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  모든 로봇이 "같은 물리 법칙"을 따름:                                   │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  F = ma (뉴턴 법칙)                                              │   │
│  │  τ = I × α (토크 = 관성모멘트 × 각가속도)                        │   │
│  │                                                                   │   │
│  │  → 같은 공식을 16개 로봇에 동시 적용 가능!                       │   │
│  │  → GPU의 병렬 처리에 완벽히 적합                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  성능 비교:                                                             │
│  ├── CPU로 16개 환경: 16 × 0.01초 = 0.16초/step                        │
│  └── GPU로 16개 환경: 1 × 0.01초 = 0.01초/step  (16배 빠름!)           │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 2. PhysX 물리 엔진

### 물리 엔진이란?

```
┌─────────────────────────────────────────────────────────────────────────┐
│  물리 엔진 = "가상 세계의 물리 법칙을 계산하는 소프트웨어"               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  현실 세계:                                                             │
│  ┌─────────────────────────────────────────────────┐                   │
│  │  공을 던지면 → 중력으로 포물선 → 바닥에 충돌 → 튕김                │
│  │  이건 자연이 알아서 계산해줌 (물리 법칙)                            │
│  └─────────────────────────────────────────────────┘                   │
│                                                                         │
│  가상 세계 (시뮬레이션):                                                │
│  ┌─────────────────────────────────────────────────┐                   │
│  │  공을 던지면 → ???                                                 │
│  │  컴퓨터는 물리 법칙을 모름!                                        │
│  │  → 물리 엔진이 계산해줘야 함                                       │
│  └─────────────────────────────────────────────────┘                   │
│                                                                         │
│  물리 엔진이 하는 일:                                                   │
│  1. 중력 적용: 모든 물체에 F = mg 적용                                 │
│  2. 충돌 감지: 두 물체가 닿았는지 확인                                 │
│  3. 충돌 해결: 닿았으면 튕겨내기                                       │
│  4. 관절 제약: 로봇 팔이 연결된 대로만 움직이게                        │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### PhysX 소개

```
┌─────────────────────────────────────────────────────────────────────────┐
│  PhysX = NVIDIA가 만든 물리 엔진                                        │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  역사:                                                                  │
│  ├── 2004년: AGEIA가 개발 (게임용)                                     │
│  ├── 2008년: NVIDIA가 인수                                             │
│  ├── 2018년: 오픈소스로 공개                                           │
│  └── 현재: Isaac Sim/Lab의 핵심 물리 엔진                              │
│                                                                         │
│  PhysX가 계산하는 것들:                                                 │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  1. 강체 역학 (Rigid Body Dynamics)                              │   │
│  │     - 로봇 링크들의 위치, 속도, 가속도                           │   │
│  │     - F = ma, τ = Iα 적용                                        │   │
│  │                                                                   │   │
│  │  2. 관절 역학 (Joint Dynamics)                                   │   │
│  │     - 관절 토크 → 회전 각도 변환                                 │   │
│  │     - 관절 제한 (최대/최소 각도)                                 │   │
│  │                                                                   │   │
│  │  3. 충돌 처리 (Collision)                                        │   │
│  │     - 그리퍼가 물체에 닿았는지 감지                              │   │
│  │     - 닿으면 접촉력 계산                                         │   │
│  │                                                                   │   │
│  │  4. 마찰/반발 (Friction/Restitution)                             │   │
│  │     - 물체가 미끄러지는 정도                                     │   │
│  │     - 물체가 튕기는 정도                                         │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### PhysX CPU vs GPU 모드

```
┌─────────────────────────────────────────────────────────────────────────┐
│  PhysX는 CPU와 GPU 모드 둘 다 지원                                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  CPU 모드:                                                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  ┌─────┐                                                        │   │
│  │  │ CPU │ → 로봇1 계산 → 로봇2 계산 → 로봇3 계산 → ...          │   │
│  │  └─────┘   (순차적)                                             │   │
│  │                                                                   │   │
│  │  장점: 아무 컴퓨터에서 동작                                      │   │
│  │  단점: 환경 많아지면 느려짐                                      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  GPU 모드 (Isaac Lab이 사용):                                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  ┌─────┐   ┌─────────────────────────────────────────┐          │   │
│  │  │ CPU │ → │ GPU                                     │          │   │
│  │  └─────┘   │  로봇1 ┐                                │          │   │
│  │    설정    │  로봇2 ├─ 동시 계산!                    │          │   │
│  │    전달    │  로봇3 │                                │          │   │
│  │            │  ...   ┘                                │          │   │
│  │            └─────────────────────────────────────────┘          │   │
│  │                                                                   │   │
│  │  장점: 환경 수에 관계없이 빠름                                   │   │
│  │  단점: NVIDIA GPU 필요                                          │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### PhysX GPU 시뮬레이션 한 스텝의 내부 동작

```
┌─────────────────────────────────────────────────────────────────────────┐
│  PhysX GPU 시뮬레이션 파이프라인                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  [Step 1] Broad-Phase 충돌 감지 (AABB)                                  │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  모든 물체를 박스(AABB)로 감싸서 빠르게 "혹시 충돌?" 체크        │   │
│  │                                                                   │   │
│  │    ┌───────┐                                                     │   │
│  │    │ ┌───┐ │  ← AABB (Axis-Aligned Bounding Box)                │   │
│  │    │ │ ○ │ │     실제 물체를 감싸는 직육면체                     │   │
│  │    │ └───┘ │                                                     │   │
│  │    └───────┘                                                     │   │
│  │                                                                   │   │
│  │  GPU에서: 모든 물체 쌍의 AABB 겹침을 동시에 체크                 │   │
│  │  16개 환경 × 7개 물체 = 112개 물체의 모든 쌍을 병렬 처리        │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                           │                                             │
│                           ▼                                             │
│  [Step 2] Narrow-Phase 충돌 감지 (정밀)                                 │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  AABB가 겹친 쌍만 → 실제로 닿았는지 정밀 계산                    │   │
│  │                                                                   │   │
│  │    ┌─────┐  ┌─────┐                                              │   │
│  │    │     │  │     │  AABB 겹침 → 정밀 검사                       │   │
│  │    │  ○  │  │  □  │  → 실제 충돌점, 충돌 법선 계산               │   │
│  │    │     │  │     │                                              │   │
│  │    └─────┘  └─────┘                                              │   │
│  │                                                                   │   │
│  │  GPU에서: 겹친 쌍들의 정밀 충돌 검사를 동시에                    │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                           │                                             │
│                           ▼                                             │
│  [Step 3] 제약조건 해결 (Solver)                                        │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  - 관절 제약: 로봇 팔이 연결된 대로만 움직이게                   │   │
│  │  - 접촉 제약: 물체가 서로 뚫고 지나가지 않게                     │   │
│  │  - 마찰력 적용: 미끄러짐 계산                                    │   │
│  │                                                                   │   │
│  │  GPU에서: 모든 제약조건을 반복적으로 풀기 (4~8회 반복)           │   │
│  │           → 이게 가장 계산량 많음!                               │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                           │                                             │
│                           ▼                                             │
│  [Step 4] 적분 (Integration)                                            │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  힘 → 가속도 → 속도 → 위치 업데이트                             │   │
│  │                                                                   │   │
│  │  a = F/m                                                         │   │
│  │  v_new = v_old + a × dt                                          │   │
│  │  x_new = x_old + v × dt                                          │   │
│  │                                                                   │   │
│  │  GPU에서: 모든 물체의 위치를 동시에 업데이트                     │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 3. Warp JIT 컴파일

### 문제 상황

```
┌─────────────────────────────────────────────────────────────────────────┐
│  문제: Python 코드를 GPU에서 실행하고 싶다!                              │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  GPU 프로그래밍의 전통적인 방법:                                        │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  CUDA C++ 코드를 직접 작성해야 함                                │   │
│  │                                                                   │   │
│  │  __global__ void compute_reward(float* joint_pos,                │   │
│  │                                  float* object_pos,              │   │
│  │                                  float* rewards,                 │   │
│  │                                  int n) {                        │   │
│  │      int idx = blockIdx.x * blockDim.x + threadIdx.x;           │   │
│  │      if (idx < n) {                                              │   │
│  │          float dx = joint_pos[idx*3] - object_pos[idx*3];       │   │
│  │          float dy = joint_pos[idx*3+1] - object_pos[idx*3+1];   │   │
│  │          float dz = joint_pos[idx*3+2] - object_pos[idx*3+2];   │   │
│  │          rewards[idx] = -sqrtf(dx*dx + dy*dy + dz*dz);          │   │
│  │      }                                                           │   │
│  │  }                                                               │   │
│  │                                                                   │   │
│  │  → 복잡함! 메모리 관리 어려움! 디버깅 힘듦!                      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  연구자들의 바람:                                                       │
│  "Python으로 편하게 쓰고, GPU에서 빠르게 실행되면 좋겠다!"             │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### Warp의 해결책

```
┌─────────────────────────────────────────────────────────────────────────┐
│  Warp = NVIDIA가 만든 "Python → GPU 자동 변환" 라이브러리               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  Warp 코드 (Python처럼 생김):                                           │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  import warp as wp                                               │   │
│  │                                                                   │   │
│  │  @wp.kernel  # ← 이 데코레이터가 마법!                           │   │
│  │  def compute_reward(joint_pos: wp.array(dtype=wp.vec3),         │   │
│  │                     object_pos: wp.array(dtype=wp.vec3),        │   │
│  │                     rewards: wp.array(dtype=float)):            │   │
│  │      idx = wp.tid()  # 현재 스레드 ID                            │   │
│  │      diff = joint_pos[idx] - object_pos[idx]                    │   │
│  │      rewards[idx] = -wp.length(diff)                            │   │
│  │                                                                   │   │
│  │  → 훨씬 간단! Python 문법 그대로!                                │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### JIT (Just-In-Time) 컴파일이란?

```
┌─────────────────────────────────────────────────────────────────────────┐
│  JIT = "실행할 때 그 자리에서 기계어로 변환"                             │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  전통적 컴파일 (AOT = Ahead-Of-Time):                                   │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  [소스코드] → [컴파일러] → [실행파일] → 나중에 실행              │   │
│  │     .cpp         gcc         .exe                                │   │
│  │                                                                   │   │
│  │  단점: 미리 컴파일해야 함, 컴파일 시간 필요                      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  JIT 컴파일:                                                            │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  [Python 코드] → 실행 시점에 → [GPU 기계어] → 바로 실행          │   │
│  │                                                                   │   │
│  │  @wp.kernel                                                      │   │
│  │  def my_func():        ───┐                                      │   │
│  │      ...                  │ 첫 호출 시                           │   │
│  │                           ▼                                      │   │
│  │  wp.launch(my_func) ──→ CUDA PTX 생성 ──→ GPU 실행              │   │
│  │                           (0.1초)           (매우 빠름)          │   │
│  │                                                                   │   │
│  │  장점: 코드 수정 → 바로 실행 (개발 편함)                         │   │
│  │  단점: 첫 호출이 약간 느림 (컴파일 시간)                         │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### Warp JIT 동작 과정 상세

```
┌─────────────────────────────────────────────────────────────────────────┐
│  Warp JIT 컴파일 단계별 설명                                             │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  [단계 1] Python 코드 작성                                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  @wp.kernel                                                      │   │
│  │  def compute_reward(positions, rewards):                        │   │
│  │      i = wp.tid()                                                │   │
│  │      rewards[i] = -wp.length(positions[i])                      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                           │                                             │
│                           ▼ 첫 호출 시                                  │
│  [단계 2] Python AST 분석                                               │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  Warp가 함수의 구조를 분석:                                      │   │
│  │  - 매개변수 타입 확인                                            │   │
│  │  - 연산 종류 파악 (덧셈, 뺄셈, 함수 호출 등)                     │   │
│  │  - 반복문, 조건문 구조 파악                                      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                           │                                             │
│                           ▼                                             │
│  [단계 3] CUDA PTX 생성                                                 │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  PTX = Parallel Thread Execution (GPU 중간 언어)                 │   │
│  │                                                                   │   │
│  │  .visible .entry compute_reward(                                 │   │
│  │      .param .u64 positions,                                      │   │
│  │      .param .u64 rewards                                         │   │
│  │  ) {                                                             │   │
│  │      mov.u32 %r1, %tid.x;           // i = wp.tid()             │   │
│  │      ld.global.v3.f32 %f1, [%rd1];  // positions[i] 로드        │   │
│  │      // ... length 계산 ...                                      │   │
│  │      st.global.f32 [%rd2], %f2;     // rewards[i] 저장          │   │
│  │  }                                                               │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                           │                                             │
│                           ▼                                             │
│  [단계 4] GPU 기계어로 최종 컴파일                                      │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  NVIDIA 드라이버가 PTX → GPU 기계어 변환                         │   │
│  │  (GPU 아키텍처에 최적화됨: Ada, Ampere, etc.)                    │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                           │                                             │
│                           ▼                                             │
│  [단계 5] GPU 실행                                                      │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  8,448개 CUDA 코어가 동시에 실행!                                │   │
│  │  각 코어가 하나의 환경/물체 담당                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  두 번째 호출부터: 단계 2-4 건너뜀 → 바로 실행 (매우 빠름)             │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 4. Isaac Lab 통합 구조

### PhysX + Warp + PyTorch 협업

```
┌─────────────────────────────────────────────────────────────────────────┐
│  Isaac Lab GPU 병렬 처리 구조                                            │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  CPU 측                          GPU 측                                 │
│  ┌──────────┐                   ┌─────────────────────────────────┐    │
│  │ Python   │   ──Warp JIT──►  │  CUDA Kernels                   │    │
│  │ 환경코드  │                   │  ┌─────┬─────┬─────┬─────┐     │    │
│  └──────────┘                   │  │Env0 │Env1 │Env2 │...  │     │    │
│                                 │  │     │     │     │     │     │    │
│  ┌──────────┐                   │  │Robot│Robot│Robot│Robot│     │    │
│  │ PhysX    │   ──GPU API──►   │  │Phys │Phys │Phys │Phys │     │    │
│  │ 설정     │                   │  └─────┴─────┴─────┴─────┘     │    │
│  └──────────┘                   │         ▼                       │    │
│                                 │  ┌─────────────────────┐        │    │
│  ┌──────────┐                   │  │ Tensor 연산 (PyTorch)│        │    │
│  │ RL 정책  │   ◄──Tensor──    │  │ - Observation        │        │    │
│  │ (신경망) │                   │  │ - Action            │        │    │
│  └──────────┘                   │  │ - Reward            │        │    │
│                                 │  └─────────────────────┘        │    │
│                                 └─────────────────────────────────┘    │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 전체 데이터 흐름

```
┌─────────────────────────────────────────────────────────────────────────┐
│  Isaac Lab: PhysX + Warp + PyTorch의 협업                               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  CPU 영역                           GPU 영역 (VRAM)                     │
│  ┌─────────────┐                   ┌───────────────────────────────┐   │
│  │             │                   │                               │   │
│  │ Python      │   환경 설정       │  ┌───────────────────────┐   │   │
│  │ 스크립트    │ ─────────────────→│  │ PhysX GPU Scene       │   │   │
│  │             │   (1회)           │  │ - 16개 환경의 물체들  │   │   │
│  │ - 환경 정의 │                   │  │ - 관절 제약조건       │   │   │
│  │ - 보상 정의 │                   │  │ - 충돌 설정           │   │   │
│  │ - 학습 설정 │                   │  └───────────────────────┘   │   │
│  │             │                   │             │                 │   │
│  └─────────────┘                   │             │ 물리 시뮬레이션 │   │
│        │                           │             ▼ (매 스텝)       │   │
│        │                           │  ┌───────────────────────┐   │   │
│        │                           │  │ 물리 상태 Tensor      │   │   │
│        │                           │  │ - joint_pos [16, 4]   │   │   │
│        │                           │  │ - joint_vel [16, 4]   │   │   │
│        │                           │  │ - body_pos [16, 7, 3] │   │   │
│        │                           │  └───────────────────────┘   │   │
│        │                           │             │                 │   │
│        │ 보상 함수                 │             │ Warp 커널      │   │
│        │ (Python으로 정의)         │             ▼ (GPU 실행)     │   │
│        │                           │  ┌───────────────────────┐   │   │
│        └──→ Warp JIT 컴파일 ──────→│  │ Observation [16, 15] │   │   │
│                                    │  │ Reward [16, 1]        │   │   │
│                                    │  └───────────────────────┘   │   │
│                                    │             │                 │   │
│                                    │             │ PyTorch        │   │
│                                    │             ▼ (GPU 추론)     │   │
│                                    │  ┌───────────────────────┐   │   │
│                                    │  │ Policy Network        │   │   │
│                                    │  │ [16, 15] → [16, 4]    │   │   │
│                                    │  │ (MLP 신경망)          │   │   │
│                                    │  └───────────────────────┘   │   │
│                                    │             │                 │   │
│                                    │             │ Action         │   │
│                                    │             ▼                 │   │
│                                    │  ┌───────────────────────┐   │   │
│                                    │  │ PhysX에 토크 적용     │   │   │
│                                    │  │ 다음 스텝 시뮬레이션  │   │   │
│                                    │  └───────────────────────┘   │   │
│                                    │                               │   │
│                                    └───────────────────────────────┘   │
│                                                                         │
│  핵심: 데이터가 GPU를 떠나지 않음! (CPU-GPU 전송 없음)                  │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 실제 데이터 흐름 (우리 환경 기준)

```
┌─────────────────────────────────────────────────────────────────────────┐
│  Step 1: 16개 환경 동시 시뮬레이션                                       │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  GPU Memory Layout:                                                     │
│  ┌──────────────────────────────────────────────────────────┐          │
│  │ joint_positions: Tensor[16, 4]  ← 16환경 × 4관절         │          │
│  │ joint_velocities: Tensor[16, 4]                          │          │
│  │ ee_positions: Tensor[16, 3]     ← End-Effector 위치      │          │
│  │ object_positions: Tensor[16, 3] ← 물체 위치 (Oracle)     │          │
│  │ grasp_states: Tensor[16, 1]     ← 그립 상태              │          │
│  └──────────────────────────────────────────────────────────┘          │
│                           │                                             │
│                           ▼                                             │
│  ┌──────────────────────────────────────────────────────────┐          │
│  │ Observation Tensor: [16, 15]                              │          │
│  │ = concat(joint_pos, joint_vel, ee_pos, obj_pos, grasp)   │          │
│  └──────────────────────────────────────────────────────────┘          │
│                           │                                             │
│                           ▼                                             │
│  ┌──────────────────────────────────────────────────────────┐          │
│  │ Policy Network (MLP) - GPU에서 배치 추론                   │          │
│  │ Input: [16, 15] → Hidden: [16, 128] → Output: [16, 4]    │          │
│  └──────────────────────────────────────────────────────────┘          │
│                           │                                             │
│                           ▼                                             │
│  ┌──────────────────────────────────────────────────────────┐          │
│  │ Actions: Tensor[16, 4] → 16개 환경에 동시 적용            │          │
│  └──────────────────────────────────────────────────────────┘          │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 구체적인 코드 예시 (우리 환경)

```python
# Isaac Lab 환경 정의 (Python)
class RoArmGraspingEnv:

    def __init__(self):
        # PhysX GPU Scene 설정
        self.sim = SimulationContext(
            device="cuda:0",  # GPU 사용
            physics_dt=1/120,
        )

        # 16개 환경 생성 - GPU 메모리에 한 번에!
        self.num_envs = 16
        self.robot = Articulation(cfg, num_envs=16)  # 16개 로봇
        self.object = RigidBody(cfg, num_envs=16)    # 16개 물체

    def step(self, actions):
        # actions: Tensor [16, 4] - GPU에 있음

        # 1. 액션을 토크로 변환 후 적용 (GPU)
        self.robot.set_joint_efforts(actions * self.torque_scale)

        # 2. PhysX 시뮬레이션 1스텝 (GPU)
        self.sim.step()  # ← PhysX GPU가 16개 환경 동시 계산

        # 3. 관측값 계산 (GPU - Warp 또는 PyTorch)
        obs = self._compute_observations()  # [16, 15]

        # 4. 보상 계산 (GPU)
        rewards = self._compute_rewards()   # [16, 1]

        return obs, rewards

    def _compute_observations(self):
        # 모든 연산이 GPU Tensor로 수행됨
        joint_pos = self.robot.get_joint_positions()  # [16, 4] GPU Tensor
        joint_vel = self.robot.get_joint_velocities() # [16, 4]
        ee_pos = self.robot.get_ee_position()         # [16, 3]
        obj_pos = self.object.get_position()          # [16, 3]

        # torch.cat도 GPU에서 실행
        obs = torch.cat([joint_pos, joint_vel, ee_pos, obj_pos, grasp], dim=1)
        return obs  # [16, 15] - 여전히 GPU에 있음!
```

---

## 5. PPO 알고리즘

### PPO가 뭔가?

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    PPO 알고리즘 핵심 아이디어                            │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  기존 Policy Gradient 문제:                                             │
│  ┌─────────────────────────────────────────────────────────┐           │
│  │  정책을 너무 많이 바꾸면 → 학습 불안정 (발산)            │           │
│  │  정책을 조금만 바꾸면   → 학습 너무 느림                 │           │
│  └─────────────────────────────────────────────────────────┘           │
│                           │                                             │
│                           ▼                                             │
│  PPO 해결책: "적당히" 바꾸자!                                           │
│  ┌─────────────────────────────────────────────────────────┐           │
│  │  Clipping: 정책 변화량을 [1-ε, 1+ε] 범위로 제한          │           │
│  │  ε = 0.2 (보통)                                          │           │
│  │                                                          │           │
│  │  → 안정적이면서도 효율적인 학습 가능!                    │           │
│  └─────────────────────────────────────────────────────────┘           │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### PPO 수식 (상세)

```
┌─────────────────────────────────────────────────────────────────────────┐
│  PPO-Clip Objective Function                                            │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  L^CLIP(θ) = E[ min( r(θ)·Â, clip(r(θ), 1-ε, 1+ε)·Â ) ]                │
│                                                                         │
│  where:                                                                 │
│  ┌────────────────────────────────────────────────────────────┐        │
│  │  r(θ) = π_θ(a|s) / π_θ_old(a|s)                            │        │
│  │         ↑ 새 정책의 확률 / 이전 정책의 확률 (확률 비율)     │        │
│  │                                                             │        │
│  │  Â = Advantage = Q(s,a) - V(s)                             │        │
│  │      ↑ 이 행동이 평균보다 얼마나 좋은가?                    │        │
│  │                                                             │        │
│  │  ε = 0.2 (clipping 범위)                                   │        │
│  │      ↑ 정책 변화를 ±20%로 제한                             │        │
│  └────────────────────────────────────────────────────────────┘        │
│                                                                         │
│  직관적 의미:                                                           │
│  ┌────────────────────────────────────────────────────────────┐        │
│  │  Â > 0 (좋은 행동):                                        │        │
│  │    → r(θ) 높이고 싶음 (이 행동 확률 높이기)                │        │
│  │    → 하지만 1.2 초과하면 clip → 너무 급격한 변화 방지      │        │
│  │                                                             │        │
│  │  Â < 0 (나쁜 행동):                                        │        │
│  │    → r(θ) 낮추고 싶음 (이 행동 확률 낮추기)                │        │
│  │    → 하지만 0.8 미만이면 clip → 너무 급격한 변화 방지      │        │
│  └────────────────────────────────────────────────────────────┘        │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 우리 환경에서 PPO 동작

```
┌─────────────────────────────────────────────────────────────────────────┐
│  RoArm Grasping에서의 PPO 학습 흐름                                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  [1] 데이터 수집 (Rollout) - 16개 환경 동시                             │
│  ┌─────────────────────────────────────────────────────────────┐       │
│  │  for step in range(24):  # rollout_length = 24              │       │
│  │      obs = env.get_observations()      # [16, 15]           │       │
│  │      actions = policy(obs)             # [16, 4]            │       │
│  │      env.step(actions)                                      │       │
│  │      rewards = env.get_rewards()       # [16, 1]            │       │
│  │      buffer.store(obs, actions, rewards, ...)               │       │
│  └─────────────────────────────────────────────────────────────┘       │
│                           │                                             │
│                           ▼                                             │
│  [2] Advantage 계산 (GAE - Generalized Advantage Estimation)            │
│  ┌─────────────────────────────────────────────────────────────┐       │
│  │  δ_t = r_t + γ·V(s_{t+1}) - V(s_t)                         │       │
│  │  Â_t = δ_t + (γλ)·δ_{t+1} + (γλ)²·δ_{t+2} + ...           │       │
│  │                                                             │       │
│  │  γ = 0.99 (discount factor)                                │       │
│  │  λ = 0.95 (GAE lambda)                                     │       │
│  └─────────────────────────────────────────────────────────────┘       │
│                           │                                             │
│                           ▼                                             │
│  [3] 정책 업데이트 (5 epochs × minibatch)                               │
│  ┌─────────────────────────────────────────────────────────────┐       │
│  │  for epoch in range(5):        # num_learning_epochs = 5    │       │
│  │      for batch in buffer.minibatches(size=16*24//4):       │       │
│  │          # PPO Clip Loss 계산                               │       │
│  │          ratio = new_prob / old_prob                        │       │
│  │          clipped = clip(ratio, 0.8, 1.2)                   │       │
│  │          loss = -min(ratio*adv, clipped*adv)               │       │
│  │          optimizer.step()                                   │       │
│  └─────────────────────────────────────────────────────────────┘       │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### PPO vs 다른 알고리즘 비교

| 특성 | PPO | SAC | TD3 | TRPO |
|------|-----|-----|-----|------|
| **타입** | On-Policy | Off-Policy | Off-Policy | On-Policy |
| **Sample 효율** | 낮음 | 높음 | 높음 | 낮음 |
| **안정성** | 높음 | 중간 | 중간 | 매우 높음 |
| **구현 난이도** | 쉬움 | 중간 | 중간 | 어려움 |
| **연속 행동** | ✅ | ✅ | ✅ | ✅ |
| **병렬 환경** | 매우 적합 | 적합 | 적합 | 매우 적합 |
| **Isaac Lab 기본** | ✅ RSL-RL | SKRL | SKRL | ❌ |

**왜 PPO를 선택했나?**
1. **병렬 환경에 최적화**: 16개 환경에서 동시에 데이터 수집 → On-Policy의 단점(낮은 sample 효율) 상쇄
2. **안정적인 학습**: Clipping으로 발산 방지 → 첫 프로젝트에 적합
3. **Isaac Lab 기본 지원**: RSL-RL이 PPO 최적화되어 있음

---

## 6. 우리 환경 설정

### SimulationCfg 설정

```python
# Isaac Lab의 SimulationCfg 설정 (우리 프로젝트)
sim: SimulationCfg = SimulationCfg(
    dt=1/120,
    device="cuda:0",           # GPU에서 물리 시뮬레이션
    physx=PhysxCfg(
        gpu_found_lost_pairs_capacity=2**21,    # GPU 충돌 감지 버퍼
        gpu_collision_stack_size=2**20,         # GPU 충돌 스택
        gpu_max_rigid_patch_count=5 * 2**15,    # GPU 강체 패치
    )
)
```

### RSL-RL PPO 설정

```python
# Isaac Lab RSL-RL PPO 설정 (우리 환경)
@configclass
class RoArmGraspingPPOCfg:
    # Actor-Critic 네트워크
    policy = {
        "class_name": "ActorCritic",
        "hidden_dims": [128, 128, 128],  # 3층 MLP
        "activation": "elu",
    }

    # PPO 하이퍼파라미터
    algorithm = {
        "clip_param": 0.2,           # ε = 0.2 (정책 변화 제한)
        "entropy_coef": 0.01,        # 탐험 장려
        "value_loss_coef": 1.0,      # Value function 손실 가중치
        "max_grad_norm": 1.0,        # Gradient clipping
        "gamma": 0.99,               # Discount factor
        "lam": 0.95,                 # GAE lambda
    }

    # 학습 설정
    runner = {
        "num_steps_per_env": 24,     # Rollout length
        "num_learning_epochs": 5,    # PPO epochs
        "num_minibatches": 4,        # Minibatch 수
        "learning_rate": 1e-3,
    }
```

### 하드웨어 사양

| 항목 | 사양 |
|------|------|
| GPU | RTX 4070 Ti SUPER |
| CUDA 코어 | 8,448개 |
| VRAM | 16GB GDDR6X |
| 메모리 대역폭 | 672 GB/s |
| 환경 수 | 16개 (4×4 Grid) |
| 시뮬레이션 Hz | 120Hz |
| 학습 Hz | ~5Hz (24 steps per update) |

---

## 7. 예상 질문 & 답변

### Q1: "GPU 스레드 하나가 환경 하나를 담당하나요?"

```
A: 아닙니다. PhysX GPU는 더 세밀하게 병렬화됩니다.

   - 1개 환경 내에서도 여러 강체(rigid body)의 물리 연산이 병렬 처리
   - 충돌 감지는 환경 간 + 환경 내 모두 병렬
   - 우리 환경: 16환경 × (로봇 5링크 + 테이블 + 물체) = 112개 강체 동시 시뮬레이션
```

### Q2: "CPU-GPU 데이터 전송 오버헤드는 어떻게 처리하나요?"

```
A: Isaac Lab은 "GPU-first" 설계입니다.

   - Observation, Action, Reward 모두 GPU Tensor로 유지
   - CPU로 복사하지 않고 PyTorch가 GPU에서 직접 연산
   - 유일하게 CPU 사용: 로깅, 체크포인트 저장 시
```

### Q3: "메모리 병목은 없나요? 16개면 적은 것 아닌가요?"

```
A: RTX 4070 Ti SUPER (16GB VRAM) 기준:

   - 1개 환경당 약 200-300MB 사용
   - 16개 환경 ≈ 4-5GB
   - 정책 네트워크 + 버퍼 ≈ 1-2GB
   - 총 6-7GB 사용 → 16GB 중 여유 있음
   - 32개, 64개도 가능하지만 우리 로봇 복잡도에는 16개가 적절
```

### Q4: "Warp 없이 그냥 PyTorch로 하면 안 되나요?"

```
A: 할 수 있지만, Warp가 특정 작업에 더 효율적입니다.

   PyTorch 장점:
   - 신경망 학습에 최적화 (자동 미분)
   - 큰 행렬 연산에 효율적

   Warp 장점:
   - 물리 시뮬레이션 특화 (충돌 감지, 레이캐스팅)
   - 복잡한 인덱싱, 조건문이 있는 커널에 효율적
   - PhysX와 직접 연동 (메모리 복사 없음)

   Isaac Lab의 선택:
   - 물리: PhysX + Warp
   - 신경망: PyTorch
   - 서로 GPU Tensor로 데이터 공유 (복사 없음)
```

### Q5: "CPU-GPU 데이터 전송이 전혀 없나요?"

```
A: 거의 없지만, 완전히 없지는 않습니다.

   GPU에서만 처리되는 것 (전송 없음):
   ✅ 물리 시뮬레이션 (PhysX)
   ✅ 관측값 계산 (Warp/PyTorch)
   ✅ 보상 계산 (Warp/PyTorch)
   ✅ 정책 추론 (PyTorch)
   ✅ 그래디언트 계산 (PyTorch)

   CPU로 전송되는 것 (불가피):
   ⚠️ 로깅: 학습 진행 상황 출력 (매 100스텝 정도)
   ⚠️ 체크포인트: 모델 저장 (매 500스텝 정도)
   ⚠️ 에피소드 통계: 성공률, 평균 보상 (매 에피소드 끝)

   비율: 학습 중 99%+ 연산은 GPU에서 처리
```

---

## 발표용 요약 (1분 버전)

```
"GPU 병렬 처리 구조를 설명드리겠습니다.

Isaac Lab은 세 가지 기술을 조합합니다:

첫째, PhysX GPU입니다.
NVIDIA의 물리 엔진으로, 16개 환경의 충돌 감지, 관절 역학,
접촉력 계산을 GPU에서 동시에 처리합니다.

둘째, Warp입니다.
Python 코드를 GPU 커널로 JIT 컴파일하는 기술입니다.
관측값, 보상 함수를 Python으로 편하게 작성하면
실행 시점에 CUDA 코드로 변환되어 GPU에서 실행됩니다.

셋째, PyTorch입니다.
정책 네트워크의 추론과 학습을 GPU에서 처리합니다.

핵심은 '데이터가 GPU를 떠나지 않는다'는 점입니다.
PhysX → Warp → PyTorch 모두 GPU Tensor를 공유해서
CPU-GPU 전송 오버헤드가 없습니다.

RTX 4070 Ti SUPER의 8,448개 CUDA 코어가
16개 환경을 동시에 시뮬레이션하면서
실시간으로 정책을 학습합니다."
```

---

## 참고 자료

- [NVIDIA PhysX Documentation](https://nvidia-omniverse.github.io/PhysX/)
- [NVIDIA Warp Documentation](https://nvidia.github.io/warp/)
- [Isaac Lab Documentation](https://isaac-sim.github.io/IsaacLab/)
- [PPO Paper (Schulman et al., 2017)](https://arxiv.org/abs/1707.06347)
- [RSL-RL GitHub](https://github.com/leggedrobotics/rsl_rl)

---

*"The key is that data never leaves the GPU" - Isaac Lab Design Philosophy*
