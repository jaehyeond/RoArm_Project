# SmolVLA Fine-tuning 결과 보고서

**날짜**: 2026-02-04
**프로젝트**: RoArm M3 Pro VLA 학습
**태스크**: "Pick up the white box"

---

## 1. 실험 개요

### 목표
- SmolVLA (Vision-Language-Action) 모델을 RoArm M3 로봇 데이터로 fine-tuning
- 1 에피소드 데이터로 빠른 검증 (오버피팅 예상)

### 데이터셋
| 항목 | 값 |
|------|-----|
| 에피소드 수 | 1 |
| 프레임 수 | 222 |
| FPS | 30 |
| 이미지 해상도 | 1280×720 (RGB) |
| 관절 수 | 6 (Base, Shoulder, Elbow, Wrist Pitch, Wrist Roll, Gripper) |
| 태스크 | "Pick up the white box" |

---

## 2. 모델 구성

### SmolVLA 아키텍처
```
SmolVLM2-500M-Video-Instruct (Frozen)
├── Vision Encoder (SigLIP) - Frozen
├── Language Model (SmolLM2) - Frozen
└── Action Expert (Trainable) ← Fine-tuning 대상
```

### 파라미터 구성
| 구분 | 파라미터 수 | 비율 |
|------|------------|------|
| **전체** | 450,046,176 | 100% |
| **학습 가능** | 99,880,992 | **22.2%** |
| Vision Encoder | ~300M | Frozen |
| Language Model | ~50M | Frozen |
| Action Expert | ~100M | **Trainable** |

### 학습 설정
| 하이퍼파라미터 | 값 |
|---------------|-----|
| Epochs | 50 |
| Batch Size | 1 |
| Learning Rate | 1e-4 |
| Optimizer | AdamW |
| Action Chunk Size | 20 |
| N Action Steps | 10 |

---

## 3. 학습 결과

### 손실 함수 (Loss)
- **Loss 계산**: MSE (Mean Squared Error)
- **대상**: 20개 미래 액션 × 6 DOF 관절 예측

### Loss 추이
| Epoch | Loss | 감소율 | 상태 |
|-------|------|--------|------|
| 1 | 248.46 | - | 초기 (랜덤) |
| 5 | 37.86 | 84.8% | 급격한 감소 |
| 10 | 5.79 | 97.7% | 빠른 학습 |
| 20 | 1.86 | 99.3% | 수렴 시작 |
| 30 | 1.54 | 99.4% | 안정화 |
| 40 | 1.08 | 99.6% | 미세 조정 |
| **50** | **0.79** | **99.7%** | **최종** |

### Loss Curve (ASCII)
```
   248.5 |*
         |*
   198.9 |*
         |*
   149.3 |*
         |*
    99.8 |**
         |**
    50.2 |**
         |***
     0.7 |*************************
         +--------------------------------------------------
          Epoch 1                                   Epoch 50
```

### 학습 성능
| 메트릭 | 값 |
|--------|-----|
| 총 학습 시간 | **22.8분** |
| 에포크당 시간 | 27.4초 |
| GPU 메모리 사용 | **1.9 GB** |
| GPU | RTX 4070 SUPER (12.9 GB) |

---

## 4. 분석

### Loss 해석
- **초기 Loss (248.46)**:
  - 20개 액션 × 6 DOF = 120개 예측값
  - MSE ≈ 248 → 평균 오차 ≈ √(248/120) ≈ 1.44도/관절
  - 랜덤 예측 상태

- **최종 Loss (0.79)**:
  - MSE ≈ 0.79 → 평균 오차 ≈ √(0.79/120) ≈ 0.08도/관절
  - 학습 데이터에 거의 완벽하게 피팅

### 학습 특성
1. **급격한 초기 감소**: Epoch 1-10에서 97.7% 감소
2. **빠른 수렴**: Epoch 20 이후 안정화
3. **낮은 메모리 사용**: 12.9GB 중 1.9GB만 사용 (14.7%)

### 한계점
| 문제 | 설명 |
|------|------|
| **오버피팅** | 1 에피소드 = 데이터 암기, 일반화 불가 |
| **조건 의존성** | 학습 환경과 동일한 조건에서만 동작 |
| **실용성** | 실제 사용을 위해 50+ 에피소드 필요 |

---

## 5. 체크포인트

### 저장 위치
```
outputs/smolvla_roarm_50ep/checkpoint.pt
```

### 체크포인트 내용
```python
{
    "policy_state_dict": ...,     # 모델 가중치
    "config": SmolVLAConfig,      # 설정
    "loss_history": [248.46, ..., 0.79],  # 50개 Loss 값
    "training_info": {
        "epochs": 50,
        "total_frames": 222,
        "total_time_seconds": 1368.0,
        "final_loss": 0.7928,
        "timestamp": "2026-02-04T22:54:57"
    }
}
```

---

## 6. 발표 포인트

### 핵심 성과
1. **효율적 학습**: 4.5억 파라미터 중 22%만 학습 (Action Expert only)
2. **빠른 수렴**: 10 에포크 이내 95% Loss 감소
3. **저 GPU 사용**: 1.9GB로 학습 가능 (소비자 GPU 친화적)
4. **짧은 학습 시간**: 23분 (50 epochs)

### 기술적 의의
- Vision-Language-Action 통합 모델 검증
- 소형 VLA 모델(500M)의 로봇 적용 가능성 확인
- LeRobot 프레임워크 + RoArm M3 통합 성공

### 향후 과제
1. **데이터 확장**: 50+ 에피소드 수집 (Leader-Follower 모드)
2. **일반화 테스트**: 다양한 물체/위치에서 평가
3. **실시간 추론**: 로봇에서 실제 동작 검증

---

## 7. 재현 방법

### 환경 설정
```powershell
# 가상환경 활성화
E:\RoArm_Project\.venv\Scripts\activate

# 필요 패키지 (이미 설치됨)
pip install lerobot torch torchvision
```

### 데이터셋 경로
```
E:\RoArm_Project\lerobot_dataset_v3\roarm_m3_pick\
```

### 학습 실행
```powershell
cd E:\RoArm_Project
python train_smolvla.py
```

---

## 8. 관련 파일

| 파일 | 설명 |
|------|------|
| `train_smolvla.py` | 학습 스크립트 |
| `convert_to_lerobot_v3.py` | 데이터셋 변환 스크립트 |
| `outputs/smolvla_roarm_50ep/checkpoint.pt` | 학습된 모델 |
| `lerobot_dataset_v3/roarm_m3_pick/` | 학습 데이터셋 |

---

## 9. Dry-Run 추론 테스트 결과

### 테스트 환경
- **날짜**: 2026-02-04
- **스크립트**: `test_smolvla_inference.py --dry-run`
- **데이터**: 학습 데이터셋 샘플 사용 (카메라/로봇 없이)

### 추론 성능
| 메트릭 | 값 | 비고 |
|--------|-----|------|
| 첫 추론 시간 | 474.7 ms | 모델 웜업 (CUDA 커널 컴파일) |
| 이후 추론 시간 | 2-3 ms | **매우 빠름** |
| 실시간 가능 여부 | ✅ 가능 | 10Hz 제어 충분 |

### 예측 정확도 (L2 오차)
| Sample Index | L2 오차 | 평가 |
|--------------|---------|------|
| 0 (에피소드 시작) | **1.50** | ✅ 매우 좋음 |
| 50 | 81.32 | ❌ 높음 |
| 100 | 70.56 | ❌ 높음 |
| 150 | 42.96 | ⚠️ 중간 |
| 200 | 77.54 | ❌ 높음 |

### 상세 예측 결과

#### Sample 0 (성공 케이스)
```
입력 state:  [ 0.79, 31.73, -1.05, 68.38,  7.47,  1.93]
예측 action: [ 0.24, 31.38, -1.95, 68.59,  6.52,  1.66]
실제 action: [ 0.79, 31.73, -1.05, 68.38,  7.47,  1.93]
오차 (L2): 1.50
```

#### Sample 100 (실패 케이스)
```
입력 state:  [ 1.14, 97.29,  4.13, 80.95,  6.94, 26.89]
예측 action: [-0.14, 32.89, -2.90, 69.80,  6.43,  1.35]
실제 action: [ 1.14, 97.29,  3.96, 80.95,  6.94, 26.98]
오차 (L2): 70.56
```

### 문제 분석

#### 관찰된 현상
모든 샘플에서 예측 액션이 **거의 동일한 값**으로 수렴:
```
예측 패턴: [~0, ~31, ~-1, ~69, ~7, ~1.5]  (에피소드 초기 위치)
```

#### 원인: 1 에피소드 오버피팅
| 문제 | 설명 |
|------|------|
| **시작점 편향** | 모델이 에피소드 시작 부분만 강하게 학습 |
| **이미지 무시** | 비전 정보와 관계없이 "평균 위치" 예측 |
| **일반화 실패** | 다양한 상태에서 적절한 액션 생성 불가 |

### 결론
| 항목 | 판정 |
|------|------|
| 학습 성공 여부 | ✅ Loss 감소 (99.7%) |
| 추론 속도 | ✅ 실시간 가능 (2-3ms) |
| 일반화 성능 | ❌ 불충분 (1 에피소드 한계) |
| 실제 로봇 테스트 | ⚠️ 제한적 데모만 가능 |

### 개선 방향
1. **데이터 확장**: 10-50 에피소드 수집 필요
2. **다양성 확보**: 다양한 시작 위치, 물체 위치 포함
3. **재학습**: 더 많은 데이터로 일반화 능력 향상

---

## 10. 현재 학습 방식 분석 (중요!)

### 핵심 질문: 이미지 기반 물체 인식인가?

**답: ❌ 아니오 - 현재는 좌표값 모방 학습**

### 현재 학습 데이터 구조
| 필드 | Shape | 내용 |
|------|-------|------|
| `observation.images.top` | [3, 720, 1280] | 카메라 RGB 이미지 |
| `observation.state` | [6] | 현재 관절 각도 |
| `action` | [6] | **목표 관절 각도** |
| `task` | string | "Pick up the white box" |

### 이상적인 VLA vs 현재 상태

```
┌─────────────────────────────────────────────────────────────┐
│  이상적인 VLA 동작 (목표)                                     │
├─────────────────────────────────────────────────────────────┤
│  [이미지에서 흰 상자 인식] + [텍스트 이해]                      │
│           ↓                                                  │
│  [상자 위치 기반 액션 계산] → 물체가 어디에 있든 찾아감          │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  현재 학습된 것 (1 에피소드 오버피팅)                          │
├─────────────────────────────────────────────────────────────┤
│  [이미지 무시] + [텍스트 무시]                                 │
│           ↓                                                  │
│  [저장된 관절 시퀀스 재생] → 학습 때 움직인 경로만 따라감        │
└─────────────────────────────────────────────────────────────┘
```

### 왜 이미지를 무시하는가?

1 에피소드만 학습하면:
- 모델이 이미지/텍스트를 "조건"으로 사용하는 게 아니라
- 단순히 학습 데이터의 **관절 각도 시퀀스를 암기**
- 어떤 이미지를 넣어도 똑같은 초기 위치 예측

### 진짜 Vision 기반이 되려면?

| 조건 | 현재 | 필요 |
|------|------|------|
| 에피소드 수 | 1 | **50-100+** |
| 물체 위치 다양성 | 고정 | **매번 다른 위치** |
| 물체 종류 | 1개 (white box) | **다양한 물체** |
| 조명/배경 | 고정 | **다양한 조건** |

### 다양한 데이터가 중요한 이유

```python
# 1 에피소드: 모델이 배운 것
"Pick up the white box" → [0, 31, -1, 68, 7, 2] (암기된 좌표)

# 50+ 에피소드 (다양한 위치): 모델이 배울 수 있는 것
"Pick up the white box" + [이미지: 왼쪽에 상자] → [왼쪽으로 가는 액션]
"Pick up the white box" + [이미지: 오른쪽에 상자] → [오른쪽으로 가는 액션]
"Pick up the white box" + [이미지: 멀리 있는 상자] → [멀리 가는 액션]
```

### 결론
| 질문 | 답 |
|------|-----|
| 현재 이미지 기반으로 물체 찾아가나? | ❌ **아니오** |
| 저장된 좌표값 따라가나? | ✅ **예** (1 에피소드 암기) |
| 이미지처럼 되려면? | **다양한 위치에서 50+ 에피소드 수집** |

---

## 11. 51 에피소드 학습 결과 (batch_size=1) - 실패

### 실험 개요 (2026-02-05)
| 항목 | 값 |
|------|-----|
| 에피소드 수 | 51 |
| 프레임 수 | 13,010 |
| batch_size | 1 |
| epochs | 20 |
| warmup_steps | 100 |
| decay_steps | 500 |

### 학습 결과
| Epoch | Loss | 상태 |
|-------|------|------|
| 1 | 16.88 | 초기 |
| 10 | ~2.5 | 감소 |
| 20 | **0.96** | 최종 (94.3% 감소) |

### 문제점: "평균 액션" 출력
학습 후 추론 테스트 결과, **모든 입력에 대해 거의 동일한 값 출력**:
```
예측 패턴: [~-1.5, ~30-35, ~-1, ~65-67, ~6-7, ~1-2]
```

| Sample | L2 오차 | 평가 |
|--------|---------|------|
| 0 | 2.49 | ⚠️ OK |
| 50 | 43.2 | ❌ BAD |
| 100 | 84.1 | ❌ BAD |
| 150 | 67.3 | ❌ BAD |
| 200 | 72.8 | ❌ BAD |

### 원인 분석

#### 1. batch_size=1 문제
- **그래디언트 노이즈 극심**: 단일 샘플로 업데이트 → 방향 불안정
- **평균으로 수렴**: 노이즈가 상쇄되면서 "안전한" 평균값만 학습

#### 2. 스케줄러 설정 문제
| 항목 | LeRobot 공식 | 우리 설정 | 문제 |
|------|-------------|----------|------|
| batch_size | 64 | **1** | 그래디언트 노이즈 |
| warmup_steps | 1000 | 100 | 초기 불안정 |
| decay_steps | 30000 | 500 | 학습률 급락 |

### 핵심 교훈
1. **Loss 감소 ≠ 좋은 모델**: Loss가 0.96까지 떨어졌지만 실제로는 "평균 액션" 학습
2. **batch_size=1 금지**: 최소 8 이상 필요
3. **warmup/decay 충분히**: 급격한 변화 방지

---

## 12. 51 에피소드 재학습 (batch_size=8, load_vlm_weights=True) - 실패

### 수정된 설정 (2026-02-05 ~ 2026-02-06)

#### 시도 1: batch_size=8 (load_vlm_weights=False)
| 항목 | 이전 (실패) | 수정 |
|------|------------|------|
| batch_size | 1 | **8** |
| epochs | 20 | **30** |
| warmup_steps | 100 | **500** |
| decay_steps | 500 | **10000** |

#### 시도 2: batch_size=8 + load_vlm_weights=True
| 항목 | 값 |
|------|-----|
| batch_size | 8 |
| epochs | 30 |
| load_vlm_weights | **True** (SmolVLM2-500M-Video-Instruct) |
| freeze_vision_encoder | True |
| train_expert_only | True |

### 학습 결과 (load_vlm_weights=True)
| Epoch | Loss | Time | GPU Mem |
|-------|------|------|---------|
| 1 | 55.2257 | ~11.8min | ~7.8 GB |
| 5 | 2.3xxx | ~11.8min | ~7.8 GB |
| 10 | 1.0xxx | ~11.8min | ~7.8 GB |
| 20 | 0.6xxx | ~11.8min | ~7.8 GB |
| **30** | **0.5702** | ~11.8min | ~7.8 GB |

**총 학습 시간**: 354.7분 (~5.9시간)
**체크포인트**: `outputs/smolvla_roarm_51ep_vlm/checkpoint.pt`

### 추론 테스트 결과 - 여전히 실패

**동일한 "평균 액션" 문제 지속:**
```
예측 패턴: [~0, ~34-35, ~0, ~65-67, ~8-9, ~2-3]
(어떤 입력에도 동일한 값 출력)
```

| Sample | L2 오차 | 평가 |
|--------|---------|------|
| 0 | 2.67 | OK (시작점 근처) |
| 50 | 79.34 | BAD |
| 100 | 69.31 | BAD |
| 150 | 45.83 | BAD |
| 200 | 74.67 | BAD |

### 근본 원인 분석 (3가지)

#### 원인 1: Action Expert 랜덤 초기화 (가장 치명적)
- `SmolVLAConfig()`로 새 모델 생성 → Action Expert가 **랜덤 초기화**
- 공식 방법: `lerobot/smolvla_base`에서 **사전학습된 Action Expert** 로드
- smolvla_base는 487개 커뮤니티 데이터셋(1000만 프레임)으로 사전학습됨
- 사전학습 없이 51 에피소드로는 Action Expert 학습 불가

#### 원인 2: 정규화(Normalization) 누락
- 공식 파이프라인: `batch = preprocessor(batch)` → state/action을 MEAN_STD 정규화
- 우리 커스텀 스크립트: GPU로 이동만 (`v.to(device)`) → 정규화 없음
- 학습/추론 모두 raw space이므로 일관성은 있지만, 학습 효율이 매우 낮음

#### 원인 3: LR 스케줄러 미적용
- 공식: step 기반 cosine decay + warmup (1000 steps warmup → 30000 steps decay)
- 우리: 고정 learning rate → 초기 불안정 + 후기 과적합

### 핵심 교훈
1. **VLM 가중치만으로는 부족**: VLM이 사전학습되어도 Action Expert가 랜덤이면 소용없음
2. **커스텀 학습 스크립트의 한계**: 정규화, 스케줄러 등 공식 파이프라인의 핵심 요소 누락
3. **공식 도구 사용 필수**: `lerobot-train` CLI가 3가지 문제를 모두 자동 처리

---

## 13. 공식 lerobot-train CLI로 SmolVLA 재학습 - 진행 중

### 접근 방식 (2026-02-06)

**핵심**: `lerobot/smolvla_base` 사전학습 모델 + 공식 학습 파이프라인 사용

```bash
lerobot-train \
  --policy.pretrained_path=lerobot/smolvla_base \
  --dataset.repo_id=roarm_m3_pick \
  --dataset.root=E:/RoArm_Project/lerobot_dataset_v3 \
  --batch_size=8 \
  --steps=20000 \
  --output_dir=outputs/smolvla_official
```

### 이 방식이 해결하는 문제
| 문제 | 커스텀 스크립트 | 공식 CLI |
|------|---------------|---------|
| Action Expert | 랜덤 초기화 | **smolvla_base 사전학습** |
| 정규화 | 없음 | **MEAN_STD 자동 적용** |
| LR 스케줄러 | 없음 | **cosine decay + warmup** |
| 기타 | - | **gradient clipping, mixed precision 등** |

### Fall-back 계획
| 단계 | 조건 | 방법 |
|------|------|------|
| Step 1 | 기본 | 공식 CLI + smolvla_base |
| Step 2 | Step 1 실패시 | LoRA fine-tuning (lr=1e-3) |
| Step 3 | Step 2 실패시 | 100+ 에피소드 추가 수집 |
| Step 4 | 성공 후 | 추론 테스트 + 실제 로봇 |

### 학습 결과 (Step 1: 공식 CLI + smolvla_base)

**학습 완료: 20,000 steps (2026-02-06)**

| Step | Loss | Gradient Norm | LR |
|------|------|--------------|-----|
| 100 | 0.107 | - | warmup |
| 1000 | 0.071 | ~0.5 | ~1e-4 |
| 3000 | 0.047 | ~0.5 | ~9e-5 |
| 5000 | 0.034 | 0.49 | 8.5e-5 |
| 7000 | 0.023 | 0.46 | 7.5e-5 |
| 10000 | 0.017 | 0.35 | 5.5e-5 |
| 13000 | 0.012 | 0.28 | 3.2e-5 |
| 15000 | 0.010 | 0.23 | 1.9e-5 |
| 17000 | 0.010 | 0.22 | 8.0e-6 |
| 20000 | 0.009 | 0.20 | 2.5e-6 |

**학습 시간**: ~87분 (RTX 4070 Ti SUPER, batch_size=8)
**체크포인트**: 005000, 010000, 015000, 020000

### Windows 호환성 패치 (run_official_train.py)

| 문제 | 해결책 |
|------|--------|
| `Path("lerobot/smolvla_base")` → `lerobot\smolvla_base` | `as_posix()` 변환 monkey-patch |
| `symlink_to()` Windows 미지원 | `write_text()` 텍스트 포인터 대체 |
| Step 5000에서 symlink 크래시 | 패치 후 `--resume=true`로 재개 |

### 추론 테스트 결과 (PASS!)

**Mean Action 문제 완전 해결!**

| Sample | Predicted | Ground Truth | L2 Error |
|--------|-----------|-------------|----------|
| 0 | [1.1, 31.1, -3.8, 68.9, 7.4, 1.7] | [0.8, 31.7, -1.1, 68.4, 7.5, 1.9] | **2.92** |
| 50 | [0.9, 89.3, -2.6, 98.4, 4.7, 39.0] | [0.6, 95.3, -1.7, 97.8, 6.4, 43.5] | 7.80 |
| 100 | [1.2, 93.1, 2.0, 84.0, 6.3, 27.7] | [1.1, 97.3, 4.0, 81.0, 6.9, 27.0] | 5.64 |
| 500 | [2.5, 12.5, 33.3, 0.8, 0.4, 3.4] | [1.2, 10.2, 34.7, -1.9, 0.3, 2.0] | 4.32 |
| 5000 | [20.7, 77.5, 53.9, 50.6, 13.6, 46.2] | [21.1, 81.1, 56.0, 48.9, 15.0, 46.1] | 4.80 |
| 13000 | [5.9, 7.6, -3.9, 21.2, -3.9, 3.8] | [5.5, 6.9, -3.7, 20.7, -3.6, 3.5] | **1.01** |

**핵심 지표 비교:**

| 지표 | 이전 (batch_size=1) | 공식 CLI (20K steps) |
|------|-------|------|
| Mean L2 Error | 43-84 | **4.39** |
| Action Std | ~0 (all same) | **20.89** |
| Min L2 | ~22 | **1.01** |
| Max L2 | ~144 | **7.80** |
| 다양성 판정 | FAIL | **PASS** |

---

*Generated: 2026-02-04*
*Updated: 2026-02-06 (공식 CLI 20K steps 학습 완료, 추론 테스트 PASS)*
